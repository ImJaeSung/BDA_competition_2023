# -*- coding: utf-8 -*-
"""2023BDA공모전t2_MOMA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mhjkb3ai53LfscFJNovw3UuZGqwBeH6b

## 0. 패키지 설치 및 불러오기
"""

# !pip install pycaret # compare models 패키지 설치

# !pip install optuna # compare models 패키지 설치

# !pip install catboost # catboost 모델 패키지 설치

# !pip install ipywidgets # catboost 모델 패키지 설치

from google.colab import drive
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from scipy.stats import zscore
from scipy.stats import iqr
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier, Pool
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.model_selection import StratifiedKFold , KFold
import os
import sys
from datetime import datetime
import pickle
from pycaret.datasets import get_data
from pycaret.classification import *
import optuna
from sklearn.metrics import log_loss
from xgboost.sklearn import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score
import xgboost as xgb
from sklearn import metrics
from mlxtend.frequent_patterns import association_rules, apriori
import matplotlib.patches as mpatches

"""## 1. 데이터 불러오기"""

drive.mount('/content/drive') # 구글 드라이브 마운트

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bda_modeling/dataset/tmk_bda_train.csv', index_col = 0) # tmk_bda_train.csv 불러오기
df

"""### 데이터 나누기"""

employee = df[df['employee_yn'] == 'Y'].drop(columns = 'employee_yn') # 직원데이터
customer = df[df['employee_yn'] == 'N'].drop(columns = 'employee_yn') # 일반 고객데이터

"""##2. 데이터 파악하기

###0. 데이터 shape와 type 파악
"""

# 데이터 개수 파악
print('------total-------')
print(df.shape)
print('------employee-------')
print(employee.shape)
print('------customer-------')
print(customer.shape)

df.info() # 데이터 각 컬럼 type 파악

"""###1. 요약통계량"""

# 요약통계량 파악
print('------total-------')
display(df.describe())
print('------employee-------') # employee 요약통계량
display(employee.describe())
print('------customer-------') # customer 요약통계량
display(customer.describe())

"""###2. 결측치, 중복값 확인"""

df.isnull().sum() # 컬럼별 결측치 확인
                  # 없음

df.duplicated().sum() # 중복 데이터 확인
                      # 없음

"""## 3. 데이터 전처리

### 1. 1) EDA - employee_yn과 prime_yn에 따른 고객의 수
"""

plt.pie(df['employee_yn'].value_counts(),
        labels = ['customer', 'employee'],
        autopct = '%1.1f%%', colors = ['#0062CD', '#FF9700'])  # 직원과 일반 고객의 비율 파악

plt.pie(employee['prime_yn'].value_counts(),
        labels = ['prime', 'general'],
        autopct = '%1.1f%%',  colors = ['#0062CD', '#FF9700']) # 직원 중 prime 회원 비율 파악

print(employee['prime_yn'].value_counts()) # 직원 중 prime 회원 수 파악

plt.pie(customer['prime_yn'].value_counts(),
        labels = ['general', 'prime'],
        autopct = '%1.1f%%', colors = ['#FF9700', '#0062CD'])  # 일반 고객 중 prime 회원 비율 파악

print(customer['prime_yn'].value_counts())  # 일반 고객 중 prime 회원 수 파악

"""### 1. 2) EDA - 이상치 확인

#### customer
"""

# numeric 컬럼인 net_order_qty과 net_order_amt의 이상치 파악하기 위해 boxplot 이용
plt.subplot(1, 2, 1)
plt.boxplot(customer['net_order_qty'])
plt.xlabel('net_order_qty')
plt.subplot(1, 2, 2)
plt.boxplot(customer['net_order_amt'])
plt.xlabel('net_order_amt')

# customer에서의 이상치 확인
#z_score 값이 3이상이면서 제 1사분위수 및 제 3사분위수 기준 1.5*IQR 벗어나는 값들 모두 이상치로 판단

# z-score 확인
customer_z = customer[['net_order_qty', 'net_order_amt']].apply(zscore).apply(abs)
(customer_z > 3).sum()
customer_z[ (customer_z.net_order_qty > 3) | (customer_z.net_order_amt > 3) ]
z_index = customer_z[ (customer_z > 3).sum(axis=1) >= 1 ].index
len(z_index)

# iqr 확인
customer_iqr = customer[['net_order_qty', 'net_order_amt']]
iqr_value = customer_iqr.apply(iqr)

highest = customer.quantile(0.75) + 1.5*iqr_value
lowest = customer.quantile(0.25) - 1.5*iqr_value  #1.5*iqr 기준으로 확인

iqr_index = customer_iqr[ ((customer_iqr > highest) | (customer_iqr < lowest)).sum(axis=1) >= 1 ].index
len(iqr_index)

customer_outlier_index = pd.Index(set(list(iqr_index)).intersection(set(list(z_index))))
len(customer_outlier_index) # 위 기준대로 이상치 개수 파악

# customer에서의 이상치 확인
#z_score 값이 3이상이면서 제 1사분위수 및 제 3사분위수 기준 3*IQR 벗어나는 값들 모두 이상치로 판단

# z-score 확인
customer_z = customer[['net_order_qty', 'net_order_amt']].apply(zscore).apply(abs)
(customer_z > 3).sum()
customer_z[ (customer_z.net_order_qty > 3) | (customer_z.net_order_amt > 3) ]
z_index = customer_z[ (customer_z > 3).sum(axis=1) >= 1 ].index
len(z_index)

# iqr 확인
customer_iqr = customer[['net_order_qty', 'net_order_amt']]
iqr_value = customer_iqr.apply(iqr)

highest = customer.quantile(0.75) + 3*iqr_value #3*iqr 기준으로 확인
lowest = customer.quantile(0.25) - 3*iqr_value

iqr_index = customer_iqr[ ((customer_iqr > highest) | (customer_iqr < lowest)).sum(axis=1) >= 1 ].index
len(iqr_index)

customer_outlier_index = pd.Index(set(list(iqr_index)).intersection(set(list(z_index))))
len(customer_outlier_index) # 위 기준대로 이상치 개수 파악

"""####employee"""

# numeric 컬럼인 net_order_qty과 net_order_amt의 이상치 파악하기 위해 boxplot 이용
plt.subplot(1, 2, 1)
plt.boxplot(employee['net_order_qty'])
plt.xlabel('net_order_qty')
plt.subplot(1, 2, 2)
plt.boxplot(employee['net_order_amt'])
plt.xlabel('net_order_amt')

# employee에서의 이상치 확인
#z_score 값이 3이상이면서 제 1사분위수 및 제 3사분위수 기준 1.5*IQR 벗어나는 값들 모두 이상치로 판단

# z-score 확인
employee_z = employee[['net_order_qty', 'net_order_amt']].apply(zscore).apply(abs)
(employee_z > 3).sum()
employee_z[ (employee_z.net_order_qty > 3) | (employee_z.net_order_amt > 3) ]
z_index = employee_z[ (employee_z > 3).sum(axis=1) >= 1 ].index
len(z_index)

# iqr 확인
employee_iqr = employee[['net_order_qty', 'net_order_amt']]
iqr_value = employee_iqr.apply(iqr)

highest = employee.quantile(0.75) + 1.5*iqr_value #1.5*iqr 기준으로 확인
lowest = employee.quantile(0.25) - 1.5*iqr_value

iqr_index = employee_iqr[ ((employee_iqr > highest) | (employee_iqr < lowest)).sum(axis=1) >= 1 ].index
len(iqr_index)

employee_outlier_index = pd.Index(set(list(iqr_index)).intersection(set(list(z_index))))
len(employee_outlier_index)  # 위 기준대로 이상치 개수 파악

# employee에서의 이상치 확인
#z_score 값이 3이상이면서 제 1사분위수 및 제 3사분위수 기준 3*IQR 벗어나는 값들 모두 이상치로 판단

# z-score 확인
employee_z = employee[['net_order_qty', 'net_order_amt']].apply(zscore).apply(abs)
(employee_z > 3).sum()
employee_z[ (employee_z.net_order_qty > 3) | (employee_z.net_order_amt > 3) ]
z_index = employee_z[ (employee_z > 3).sum(axis=1) >= 1 ].index
len(z_index)

# iqr 확인
employee_iqr = employee[['net_order_qty', 'net_order_amt']]
iqr_value = employee_iqr.apply(iqr)

highest = employee.quantile(0.75) + 3*iqr_value #3*iqr 기준으로 확인
lowest = employee.quantile(0.25) - 3*iqr_value

iqr_index = employee_iqr[ ((employee_iqr > highest) | (employee_iqr < lowest)).sum(axis=1) >= 1 ].index
len(iqr_index)

employee_outlier_index = pd.Index(set(list(iqr_index)).intersection(set(list(z_index))))
len(employee_outlier_index)  # 위 기준대로 이상치 개수 파악

"""### 2. order_date
- 영업일과 비영업일에 따른 prime회원과 비프라임 회원간의 구매성향 차이 파악
"""

df['order_date'] = pd.to_datetime(df['order_date'], format = '%Y%m%d')  # order_date 컬럼 datetime type으로 변환

holiday = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bda_modeling/dataset/holiday_Jan_2023.csv') # 2023년 1월 비영업일 데이터 불러오기
holiday['Date'] = pd.to_datetime(holiday['Date'], format = '%Y%m%d') # 불러온 데이터에서 Date컬럼을 datetime type으로 변환
holiday['Date']

df['holiday'] = df['order_date'].apply(lambda data : 1 if data in holiday['Date'].tolist() else 0) # holiday 컬럼에 비영업일이면 1, 영업일이면 0을 할당

employee = df[df['employee_yn'] == 'Y'].drop(columns = 'employee_yn')
customer = df[df['employee_yn'] == 'N'].drop(columns = 'employee_yn') # employee와 customer 데이터로 나누기

#employee
plt.title("Employee")
sns.set_style("white")
ax = sns.countplot(x = 'holiday', hue = 'prime_yn', data = employee) # employee에서 holiday인지 아닌지에 따른 prime회원여부 차이 파악
ax.patches[0].set_facecolor('#0062CD')
ax.patches[1].set_facecolor('#0062CD')
ax.patches[2].set_facecolor('#FF9700')
ax.patches[3].set_facecolor('#FF9700')
legend_labels = ['N', 'Y']
legend_colors = ['#0062CD', '#FF9700']
patches = [mpatches.Patch(facecolor=color) for color in legend_colors]
plt.legend(patches, legend_labels, title='prime_yn')
plt.show()

#customer
plt.title("Customer")
ax = sns.countplot(x = 'holiday', hue = 'prime_yn', data = customer)  # customer에서 holiday인지 아닌지에 따른 prime회원여부 차이 파악
ax.patches[0].set_facecolor('#0062CD')
ax.patches[1].set_facecolor('#0062CD')
ax.patches[2].set_facecolor('#FF9700')
ax.patches[3].set_facecolor('#FF9700')
legend_labels = ['N', 'Y']
legend_colors = ['#0062CD', '#FF9700']
patches = [mpatches.Patch(facecolor=color) for color in legend_colors]
plt.legend(patches, legend_labels, title='prime_yn')
plt.show()

"""### 3. 1) product_name -> set_yn
- 묶음 상품 구매 여부 파악
"""

# product_name에서 *은 곱하기의 의미이므로 X로 변경
# 마찬가지로 x와 X 둘다 곱하기로 쓰인 경우가 많아 X로 통일하여 대체
df['product_name'] = df['product_name'].str.replace("*","X")
df['product_name'] = df['product_name'].str.replace("x","X")
df['product_name']

# unique한 product_name의 개수는 3104개
unique_product = pd.DataFrame(df['product_name'].unique())
unique_product.columns = ['unique_product_name']
unique_product

# 띄어쓰기 구분 제거
unique_product['remove_space'] = unique_product['unique_product_name'].str.replace(' ', '')
unique_product

# 묶음 상품이면 1, 단일 상품이면 0
# 포, 정, 환, 캡슐 단위의 품목의 경우 여러개가 있어도 하나의 단일 상품으로 분류
pjhc = unique_product[unique_product['remove_space'].str.contains('[0-9][포,정,환,캡슐]', na = False)]
pjhc_index = unique_product[unique_product['remove_space'].str.contains('[0-9][포,정,환,캡슐]', na = False)].index
pjhc

# 포, 정, 환, 캡슐 단위의 품목 g/mgX(숫자)의 품목명의 경우 단일 품목의 묶음구매이므로 묶음상품으로 분류
pjhc2 = pjhc[pjhc['remove_space'].str.contains('gX', case = False)].reset_index(drop = True)
pjhc2_index = pjhc[pjhc['remove_space'].str.contains('gX', case = False)].index
a = re.compile("X[0-9]개")
for i in range(len(pjhc2)):
  b = a.search(list(pjhc2['remove_space'])[i])
  if b == None:
    pjhc2.loc[i,'묶음상품여부'] = 0
  else:
    pjhc2.loc[i,'묶음상품여부'] = 1
print(pjhc2.shape, pjhc2_index.shape)
pjhc2

# 포, 정, 환, 캡슐 단위의 품목 l/mlXn의 품목명의 경우 단일 품목의 묶음구매이므로 묶음상품으로 분류
pjhc21 = pjhc[pjhc['remove_space'].str.contains('lX', case = False)].reset_index(drop = True)
pjhc21_index = pjhc[pjhc['remove_space'].str.contains('lX', case = False)].index
a = re.compile("X[0-9]개")
for i in range(len(pjhc21)):
  b = a.search(list(pjhc21['remove_space'])[i])
  if b == None:
    pjhc21.loc[i,'묶음상품여부'] = 0
  else:
    pjhc21.loc[i,'묶음상품여부'] = 1
print(pjhc21.shape, pjhc21_index.shape)
pjhc21

# Product_name에 박스(BOX)가 들어있는 상품의 경우 묶음상품으로 구분
pjhc3 = pjhc.drop(index = list(pjhc2_index) + list(pjhc21_index) , inplace=False)
print(pjhc3.shape)
pjhc4 = pjhc3[pjhc3['remove_space'].str.contains('BOX', na = False)].reset_index(drop = True)
pjhc4_index = pjhc3[pjhc3['remove_space'].str.contains('BOX', na = False)].index
pjhc4['묶음상품여부'] = 1
pjhc4

# 나머지 데이터는 X(숫자)개의 형태로 되어있는 경우 묶음상품으로 분류
pjhc5 = pjhc3.drop(pjhc4_index , inplace=False).reset_index(drop = True)
for i in range(len(pjhc5)):
  b = a.search(list(pjhc5['remove_space'])[i])
  if b == None:
    pjhc5.loc[i,'묶음상품여부'] = 0
  else:
    pjhc5.loc[i,'묶음상품여부'] = 1
pjhc5

# 포, 정, 환, 캡슐에 대해서 만든 데이터프레임 모두 병합
pjhc_final = pd.concat([pjhc2, pjhc21, pjhc4, pjhc5])
print(pjhc_final.shape)
pjhc_final

# 포, 정, 환, 캡슐을 제외한 나머지 품목들에 대해서 데이터프레임 생성
nmg = unique_product.drop(pjhc_index , inplace=False)
nmg

# 품목 g/mgX(숫자)의 품목명의 경우 단일 품목의 묶음구매이므로 묶음상품으로 분류
nmg2 = nmg[nmg['remove_space'].str.contains('mgX', na = False)].reset_index(drop = True)
nmg2_index = nmg[nmg['remove_space'].str.contains('mgX', na = False)].index
a = re.compile("X[0-9]개")
for i in range(len(nmg2)):
  b = a.search(list(nmg2['remove_space'])[i])
  if b == None:
    nmg2.loc[i,'묶음상품여부'] = 0
  else:
    nmg2.loc[i,'묶음상품여부'] = 1
print(nmg2.shape, nmg2_index.shape)
nmg2

# 품목 l/mlXn의 품목명의 경우 단일 품목의 묶음구매이므로 묶음상품으로 분류
nmg21 = nmg[nmg['remove_space'].str.contains('mlX', na = False)].reset_index(drop = True)
nmg21_index = nmg[nmg['remove_space'].str.contains('mlX', na = False)].index
a = re.compile("(X[0-9]+개)|(X[0-9]+$)")
for i in range(len(nmg21)):
  b = a.search(list(nmg21['remove_space'])[i])
  if b == None:
    nmg21.loc[i,'묶음상품여부'] = 0
  else:
    nmg21.loc[i,'묶음상품여부'] = 1
print(nmg21.shape, nmg21_index.shape)
nmg21

# Product_name에 박스(BOX)가 들어있는 상품의 경우 묶음상품으로 구분
nmg3 = nmg.drop(index = list(nmg2_index) + list(nmg21_index) , inplace=False)
print(nmg3.shape)
nmg4 = nmg3[nmg3['remove_space'].str.contains('BOX', na = False)].reset_index(drop = True)
nmg4_index = nmg3[nmg3['remove_space'].str.contains('BOX', na = False)].index
nmg4['묶음상품여부'] = 1
nmg4

# 나머지 데이터는 X(숫자)개의 형태로 되어있는 경우 묶음상품으로 분류
nmg5 = nmg3.drop(nmg4_index , inplace=False).reset_index(drop = True)
a = re.compile("(X*[0-9]+개)|(X*[0-9]+$)|(X*[0-9]+입)|(X*[0-9]+EA)|(X*[0-9]+번들)|(X*[0-9]+ea)")
for i in range(len(nmg5)):
  b = a.search(list(nmg5['remove_space'])[i])
  if b == None:
    nmg5.loc[i,'묶음상품여부'] = 0
  else:
    nmg5.loc[i,'묶음상품여부'] = 1
nmg5

# 나머지 상품들에 대해서 만든 데이터프레임 모두 병합
nmg_final = pd.concat([nmg2, nmg21, nmg4, nmg5])
print(nmg_final.shape)
nmg_final

# 포, 정, 환, 캡슐에 대한 최종 데이터와 그 외 나머지 품목에 대한 최종 데이터 병합
# unique_product_name -> product_name
# 묶음상품 여부 -> set_yn
final = pd.concat([pjhc_final, nmg_final]).drop(columns = 'remove_space')
final.columns=["product_name", "set_yn"]
final

# 기존 데이터프레임과 묶음상품 여부에 대한 데이터프레임 merge로 병합
# 이 때 product_name을 기준으로 병합해서 각 제품에 set_yn이 알맞게 위치하도록 함
df = pd.merge(df, final, how = "left", on = "product_name")
employee = df[df['employee_yn'] == 'Y'].drop(columns = 'employee_yn')
customer = df[df['employee_yn'] == 'N'].drop(columns = 'employee_yn')

df

# employee
a1 = employee.groupby(['prime_yn', 'set_yn']).count().reset_index()[['prime_yn','set_yn', 'scd']] # employee의 프라임 회원 여부와 묶음상품 판매 여부에 따른 회원의 수
ysum = a1[a1['prime_yn'] == 'Y']['scd'].sum()   # 프라임 회원의 수 합계
nsum = a1[a1['prime_yn'] == 'N']['scd'].sum()   # 비프라임 회원의 수 합계
a1['sum'] = 0             # 프라임/비프라임 회원 수 합을 나타내는 sum열 생성, 0으로 채워놓음

# 전체 프라임/비프라임 회원 수에서 묶음상품 구입 / 미구입 비율 구하기
for i in range(len(a1)):
  if a1['prime_yn'][i] == 'Y':
    a1['sum'][i] = ysum           # 프라임 회원이라면 프라임 회원의 합계 출력
  if a1['prime_yn'][i] == 'N':
    a1['sum'][i] = nsum           # 비프라임 회원이라면 비프라임 회원 합계 출력
a1['ratio'] = a1['scd'] / a1['sum']   # 프라임/비프라임 회원에서 묶음상품 판매 여부에 대한 비율을 기록하는 ratio 열 생성

plt.title("Employee")
ax = sns.barplot(x = 'prime_yn', y = 'ratio', hue = 'set_yn', data = a1)
ax.patches[0].set_facecolor('#0062CD')
ax.patches[1].set_facecolor('#0062CD')
ax.patches[2].set_facecolor('#FF9700')
ax.patches[3].set_facecolor('#FF9700')

legend = plt.legend(title = 'set_yn')
for line in legend.get_lines():
    line.set_color('#0062CD')  # 원하는 색상으로 변경 가능


plt.show()

# customer
# customer의 프라임 회원 여부와 묶음상품 판매 여부에 따른 회원의 수
a2 = customer.groupby(['prime_yn', 'set_yn']).count().reset_index()[['prime_yn','set_yn', 'scd']]
ysum = a2[a2['prime_yn'] == 'Y']['scd'].sum()   # 프라임 회원의 수 합계
nsum = a2[a2['prime_yn'] == 'N']['scd'].sum()   # 비프라임 회원의 수 합계
a2['sum'] = 0            # 프라임/비프라임 회원 수 합을 나타내는 sum열 생성, 0으로 채워놓음

for i in range(len(a2)):
  if a2['prime_yn'][i] == 'Y':
    a2['sum'][i] = ysum           # 프라임 회원이라면 프라임 회원의 합계 출력
  if a2['prime_yn'][i] == 'N':
    a2['sum'][i] = nsum           # 비프라임 회원이라면 비프라임 회원 합계 출력
a2['ratio'] = a2['scd'] / a2['sum']   # 프라임/비프라임 회원에서 묶음상품 판매 여부에 대한 비율을 기록하는 ratio 열 생성

plt.title("Customer")
ax = sns.barplot(x = 'prime_yn', y = 'ratio', hue = 'set_yn',data = a2)
ax.patches[0].set_facecolor('#0062CD')
ax.patches[1].set_facecolor('#0062CD')
ax.patches[2].set_facecolor('#FF9700')
ax.patches[3].set_facecolor('#FF9700')

legend = plt.legend(title = 'set_yn')
for line in legend.get_lines():
    line.set_color('#0062CD')  # 원하는 색상으로 변경 가능

plt.show()

"""### 3. 2) product_name -> category
- 각 상품별 카테고리 추가"""

category_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bda_modeling/dataset/category_train.csv', encoding = 'utf8').drop(columns = 'Column1')
# excel에서 작업한 category 데이터 불러오기
category = category_df[['product_name', 'category']]  # product_name과 category 컬럼만 추출
category

df = pd.concat([df, category], keys = 'product_name',  axis = 1).drop(columns = ('r',  'product_name')).droplevel(axis= 1,level = 0)
# 원본 데이터와 category 데이터를 product_name 기준으로 합침
# 중복값인 product_name 컬럼 하나 삭제 후 multiindex 풀어줌
df[[]]

# employee와 customer 데이터 분리
employee = df[df['employee_yn'] == 'Y'].drop(columns = 'employee_yn').reset_index(drop = True)
customer = df[df['employee_yn'] == 'N'].drop(columns = 'employee_yn').reset_index(drop = True)

# employee 여부에 따른 prime 여부에 대해 각 데이터 추출
employee_prime = employee[employee['prime_yn'] == 'Y']
employee_general = employee[employee['prime_yn'] == 'N']
customer_prime = customer[customer['prime_yn'] == 'Y']
customer_general = customer[customer['prime_yn'] == 'N']  # 총 2*2 = 4개의 데이터프레임 추출

def encoding(x): # apriori 분석을 위해서 구현한 사용자 정의 함수
    if x<=0:     # input이 0보다 작거나 같으면 0을, 1보다 크거나 같으면 1을 return
        return 0
    if x>=1:
        return 1

datasets = [employee_prime, employee_general, customer_prime, customer_general]
datasets_name = ['employee_prime', 'employee_general', 'customer_prime', 'customer_general']

# 위에서 생성한 4개의 데이터프레임 별로 apriori 분석 후 차이를 파악
for idx, dataset in enumerate(datasets):
  Transaction_ = dataset.groupby(['scd', 'category'])['category'].count().reset_index(name ='Count')

  basket_ = Transaction_.pivot_table(index='scd', columns='category', values='Count', aggfunc='sum').fillna(0)

  # applying the function to the dataset
  basket_ = basket_.applymap(encoding)

  freq_item_ = apriori(basket_, min_support = 0.01,use_colnames = True)

  rules_ = association_rules(freq_item_, metric = "confidence", min_threshold = 0.8)
  rules_ = rules_[rules_['lift']>1]
  rules_.sort_values('confidence', ascending = False, inplace = True)
  print("===============", f'{datasets_name[idx]}' , "apriori =================")
  display(rules_.head(20))

"""###4. scd
- 한 사람당 몇가지 종류의 product를 구매했는지 파악하기 위해 scd_count 변수 추가
"""

# 한 사람이 한 번 주문을 할 때 몇 개의 품목을 구매하는 지 확인
scd_count = df.groupby('scd').count().iloc[:,0].to_frame()
scd_count.columns = ['scd_count']
scd_count

# 기존 데이터프레임과 scd를 기준으로 병합
df = pd.merge(df, scd_count, how = 'left', on = 'scd')

# employee와 customer로 데이터 나누기
employee = df[df['employee_yn'] == 'Y'].drop(columns = 'employee_yn').reset_index(drop = True)
customer = df[df['employee_yn'] == 'N'].drop(columns = 'employee_yn').reset_index(drop = True)

# employee, customer의 프라임 회원 유무와 scd_count에 따른 회원의 수 파악
employee_ = employee.groupby(['scd_count','prime_yn']).count().reset_index()[['scd_count', 'prime_yn', 'scd']]
customer_ = customer.groupby(['scd_count','prime_yn']).count().reset_index()[['scd_count', 'prime_yn', 'scd']]

# scd_count에 따른 회원의 수 파악(a1,a2)
a1 = employee_.groupby('scd_count').sum().reset_index()
a2 = customer_.groupby('scd_count').sum().reset_index()

# employee
# 프라임 회원 여부에 따른 각 scd_count에서의 비율
employee_2 = pd.merge(employee_, a1, on = 'scd_count', how = 'left')
employee_2['ratio'] = employee_2['scd_x'] / employee_2['scd_y'] * 100

# customer
# 프라임 회원 여부에 따른 각 scd_count에서의 비율
customer_2 = pd.merge(customer_, a2, on = 'scd_count', how = 'left')
customer_2['ratio'] = customer_2['scd_x'] / customer_2['scd_y'] * 100

# employee와 customer 회원의 각각의 프라임 회원 여부에 따른 인원의 총합
y_employee_sum = employee_2[employee_2['prime_yn'] == "Y"]['scd_x'].sum()
n_employee_sum = employee_2[employee_2['prime_yn'] == "N"]['scd_x'].sum()

y_customer_sum = customer_2[customer_2['prime_yn'] == "Y"]['scd_x'].sum()
n_customer_sum = customer_2[customer_2['prime_yn'] == "N"]['scd_x'].sum()

# employee
employee_2['s_ratio'] = 0
# 프라임 회원/비프라임 회원 총 수에서 각 scd_count와 프라임 회원 여부에 따른 회원수의 비율
for i in range(len(employee_2)):
  if employee_2['prime_yn'][i] == "Y": employee_2['s_ratio'][i] = employee_2['scd_x'][i] / y_employee_sum
  if employee_2['prime_yn'][i] == "N": employee_2['s_ratio'][i] = employee_2['scd_x'][i] / n_employee_sum

# customer
customer_2['s_ratio'] = 0
# 프라임 회원/비프라임 회원 총 수에서 각 scd_count와 프라임 회원 여부에 따른 회원수의 비율
for i in range(len(customer_2)):
  if customer_2['prime_yn'][i] == "Y": customer_2['s_ratio'][i] = customer_2['scd_x'][i] / y_customer_sum
  if customer_2['prime_yn'][i] == "N": customer_2['s_ratio'][i] = customer_2['scd_x'][i] / n_customer_sum

plt.figure(figsize = (20,20))
# employee
# 한 사람이 한 번 주문을 할 때 몇 개의 품목을 구매하는 지 시각화로 파악
plt.subplot(1,2,1)
plt.title("Employee's prime_yn by number of products on each purchase")
ax = sns.barplot(data = employee_2, x = 'scd_count', y = 's_ratio', hue = "prime_yn")
for i in range(43):
  ax.patches[i].set_facecolor('#0062CD')
for i in range(43,86):
  ax.patches[i].set_facecolor('#FF9700')

legend = plt.legend(title = 'prime_yn', loc='upper right')
for line in legend.get_lines():
    line.set_color('#0062CD')  # 원하는 색상으로 변경 가능
    line.set_color('#FF9700')


# customer
# 한 사람이 한 번 주문을 할 때 몇 개의 품목을 구매하는 지 시각화로 파악
plt.subplot(1,2,2)
plt.title("Customer's prime_yn by number of products on each purchase")
ax = sns.barplot(data = customer_2, x = 'scd_count', y = 's_ratio', hue = "prime_yn")
for i in range(34):
  ax.patches[i].set_facecolor('#0062CD')
for i in range(34,68):
  ax.patches[i].set_facecolor('#FF9700')

legend = plt.legend(title = 'prime_yn', loc='upper right')
for line in legend.get_lines():
    line.set_color('#0062CD')  # 원하는 색상으로 변경 가능
    line.set_color('#FF9700')

plt.show()

"""### 5. net_order_amt
- 현재 유일하게 스케일링이 되어 있는 컬럼
- 어떠한 스케일링이 적용되어 있는지를 알 수 있으면 해당 스케일링의 역변환을 통해 스케일링 전 데이터를 얻을 수 있음
"""

# 전체 데이터에서의 net_order_amt의 분포
plt.hist(df['net_order_amt'], bins=30, color='#0062CD')
plt.title('Entire dataset net_order_amt')

# 임직원 데이터에서의 net_order_amt 분포
plt.hist(employee['net_order_amt'], bins=30, color='#0062CD')
plt.title('Employee dataset net_order_amt')

# 고객 데이터에서의 net_order_amt 분포
plt.hist(customer['net_order_amt'], bins=30, color='#0062CD')
plt.title('Customer dataset net_order_amt')

print(df['net_order_amt'].describe())
print(employee['net_order_amt'].describe())
print(customer['net_order_amt'].describe())

"""- df, employee, customer 분포 확인 결과 셋 다 왼쪽으로 약간 치우쳐 있지만, 정규분포와 유사하게 평균을 중심으로 대칭인 형태가 나타남을 확인할 수 있음
- 따라서 다양한 스케일러들 중 정규분포와 유사한 결과를 나타내는 스케일러의 역변환 결과 확인

- Min-Max, MaxAbs 등 0~1값을 가지게 하는 스케일러들은 제외

"""

plt.hist(df['net_order_qty'], bins=100, color='#FF9700')
plt.title('Entire dataset net_order_qty')

df['net_order_qty'].describe()

df['net_order_qty'].sort_values(ascending=False).head(100)

"""- net_order_qty의 분포를 보면 왜도가 매우 높은 것을 확인할 수 있음
- net_order_amt는 단순 가격 데이터가 아니라 주문 금액 데이터이므로 net_order_qty에 어느 정도 비례할 것임
- 따라서 분포 또한 종 모양이 아니라 어느 정도 치우쳐진 분포가 나타날 것임
"""

amt = df['net_order_amt']

amt.describe()

# Standard Scaler의 역변환
# 스케일링 전 값의 평균과 표준편차를 알 수 없으므로 정확히 파악은 불가능
# 스케일링 전과 후의 분포 형태 비교를 위해 임의의 가중치를 주어서 계산
plt.hist((amt * (100*amt.std()) + 100000*amt.mean()), bins=30, color='#0062CD')
plt.title('Inverse Standard Scaler')

# Robust Scaler의 역변환
# 스케일링 전 값의 평균과 표준편차를 알 수 없으므로 정확히 파악은 불가능
# 스케일링 전과 후의 분포 형태 비교를 위해 임의의 가중치를 주어서 계산
plt.hist((amt * (12345*amt.describe()[6] - 123*amt.describe()[4]) + 5000*amt.describe()[5]), bins=30, color='#0062CD')
plt.title('Inverse Robust Scaler')

"""- Standard, Robust 등은 데이터의 범위를 변화시킬수는 있지만 데이터 분포의 모양은 크게 변화시키지 않음
- 따라서 net_order_qty의 분포 형태와 크게 다른 형태가 나타나므로 둘은 아닐 가능성이 높음
- 이후 exponential을 적용한 데이터에 Standard와 Robust를 적용시켜보면서 분포 형태 변화 여부를 추가로 확인
"""

# Log Scaler의 역변환 -> Exponential 함수 적용
exp = np.exp(amt)

plt.hist(exp, bins=500, color='#FF9700')
plt.title('Inverse Log Scaler (Exponential)')

"""- net_order_qty와 유사하게 왜도가 매우 큰 분포가 나타남"""

# exp에 Standard Scaler 적용해서 분포 확인
plt.hist(((exp - exp.mean()) / exp.std()), bins=500, color='#FF9700')
plt.title('Exponential Inverse Standard Scaler')

# exp에 Robust Scaler 적용해서 분포 확인
plt.hist(((exp - exp.describe()[5]) / (exp.describe()[6] - exp.describe()[4])), bins=500, color='#FF9700')
plt.title('Exponential Inverse Robust Scaler')

"""- 한 쪽으로 크게 치우친 분포에서 역시 분포의 형태를 변화시키지 못함
- 따라서 Standard, Robust는 아닐 것임

- 위 결과를 종합해보면 Standard, Min-Max, Robust, MinAbs 등 일반적으로 많이 사용되는 스케일러들은 데이터의 분포를 크게 변화시키지 못하므로 이 경우에는 사용하지 않았을 가능성이 높음
- 일반적으로 왜도가 큰 데이터의 분포를 안정화시키기 위해서는 로그 스케일링을 사용하고, 로그의 역변환인 지수변환을 적용한 결과 net_order_qty의 분포 형태와 유사한 형태를 보이므로 가장 유력함
- 하지만 이러한 결과만을 가지고 단정짓기에는 무리가 있으므로 cj 더 마켓에서의 실제 가격과 대조해보는 과정 수행

- 실제 가격과 대조해보는 과정에서 customer 데이터를 사용
- 실제 가격은 상품 1개의 가격이므로 exp / net_order_qty를 통해 price_per_unit이라는 열을 얻고, 이를 이용해서 비교
- net_order_amt는 최종 주문 금액이었으므로 이렇게 하면 적립금, 배송비 등을 고려하기 힘듦
- 만약 price_per_unit이 진짜로 상품 한 단위 당 금액이라면 각 상품별로 price_per_unit이 가장 큰 값이 실제 가격과 유사할 것임
- 왜냐하면 나머지 price_per_unit은 적립금, 쿠폰 등으로 인해 정가 대비 할인을 받았을 것이기 때문
"""

# price_per_unit 열 추가
df['price_per_unit'] = exp / df['net_order_qty']

# customer와 employee로 데이터 분리
customer = df[df['employee_yn'] == 'N'].drop(columns='employee_yn')
employee = df[df['employee_yn'] == 'Y'].drop(columns='employee_yn')

# 각 product중 price_per_unit이 max인 행 추출
customer_max = customer.groupby('product_name').max()['price_per_unit'].reset_index()
customer_max

customer_compare = pd.merge(customer, customer_max, how = "left", on = "product_name")
customer_compare

# 각 product별 price_per_unit이 최대인 데이터의 net_order_qty와 price_per_unit 추출
customer_compare_price = customer_compare[customer_compare['price_per_unit_x'] == customer_compare['price_per_unit_y']].drop(columns='price_per_unit_y').rename(columns={'price_per_unit_x' : 'price_per_unit'})
ccp = customer_compare_price.groupby(['product_name']).mean()[['net_order_qty','price_per_unit']]
ccp
# ccp.to_csv('/content/drive/MyDrive/bdaa/ccp.csv', encoding='utf-8')

# 일부 상품의 실제 가격을 찾아본 데이터
price = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bda_modeling/dataset/price_per_unit.csv')
price.head()

# 찾는데 성공한 데이터 외 나머지 데이터 제거
price.dropna(inplace=True)

# 실제 가격과 스케일링 역변환해서 얻은 값의 차이 계산
price['dif_price'] = price['true_price'] - price['price_per_unit']

# 실제 가격 대비 할인율도 계산
price['discount_rate'] = round(1 - (price['price_per_unit'] / price['true_price']), 4)

# 가격 차이를 기준으로 정렬
price.sort_values('dif_price')

# 할인율을 기준으로 정렬
price.sort_values('discount_rate')

# 가격 차이가 -1원인 데이터 확인
len(price[price['dif_price'] == -1])

"""- 위 데이터에서 dif_price가 -1인 것들이 다수 존재
- -1 차이는 스케일링 시 round등의 소수점 반올림으로 인해 충분히 발생할 수 있는 정도
- 즉 가격이 거의 일치하는 데이터가 상당 부분 있다고 볼 수 있음
- 추가로 할인율을 보면 주어진 데이터에서 최대가 45%인 것을 확인할 수 있는데 이는 1월에 진행한 '더세페' 등의 행사에서 '첫구매고객 45% 쿠폰'이나 '최대 40%할인 + 10% 추가 할인 쿠폰 뽑기' 등이 적용된 결과라고 유추가 가능
- 이를 종합해보면 net_order_amt에 로그 스케일링이 적용되었을 것이라는 가정을 하기에 근거가 충분함
- 따라서 np.exp(net_order_amt) / net_order_qty를 (할인이 적용된)상품 한 단위 당 가격(price_per_unit)으로 간주하고 분석 진행

- 이제 price_per_unit이라는 열이 프라임 회원 여부 예측에 도움이 될 것인지 확인
- 프라임 회원의 경우 비프라임 회원에 비해 적립금, 배송비 등으로 인해 같은 상품에 대해 가격이 낮을 것으로 예상
- 하지만 비프라임 회원이 따로 쿠폰을 적용한 경우 역전되는 경우도 있을 수 있음
- 따라서 같은 상품 내에 price_per_unit의 평균을 이용해서 비교
"""

df_price = df.groupby(['product_name', 'prime_yn']).mean()[['price_per_unit']]
df_price

comparison = df_price.unstack().diff(axis=1)[[('price_per_unit', 'Y')]].dropna()

comparison

# 값이 0보다 작으면 해당 상품에 대해 프라임 회원이 지불하는 금액이 더 낮음
# 다음은 전체 상품 중 그 비율을 확인하기 위한 코드
prime_rate = sum(comparison[('price_per_unit', 'Y')] < 0) / len(comparison)
non_prime_rate = sum(comparison[('price_per_unit', 'Y')] >= 0) / len(comparison)

plt.pie([prime_rate, non_prime_rate], labels=['Prime', 'Non-Prime'], autopct='%.2f%%', explode=[0.05, 0.05], colors = ["#0062CD", "#FF9700"])
plt.title("Which group's price is low within the same product?")

"""- 데이터에서 프라임 회원과 비프라임 회원 모두 구매한 이력이 있는 상품에 대해서, 해당 상품의 가격이 프라임 회원이 더 낮은 경우가 약 90.5%를 차지함
- 따라서 프라임 회원이면 동일 상품에 대해서 더 낮은 가격을 지불하는 것이 꽤나 타당한 것을 확인할 수 있고, 모델에 넣어서 분석을 진행하기에 무리가 없어보임
"""

plt.bar(x=comparison.index, height=comparison[('price_per_unit', 'Y')], color='#FF9700')
plt.xlabel('Product')
plt.ylabel('Price Difference')
plt.title('Price Difference by Product and Prime')
plt.xticks([])
plt.axhline(0, linestyle='--', linewidth=1, color='gray')
plt.show()

# 위 그림의 값이 명확하게 보이지 않아서 Price Difference의 범위를 -15000 ~ 5000으로 제한
# 핵심은 값이 0 근처에 모여있는 데이터의 부호이므로 잘 안 보이는 데이터 제거 후 다시 시각화 (즉 잘 보이게 하기 위해서)
comparison_2 = comparison[(comparison[('price_per_unit', 'Y')] > -15000) & (comparison[('price_per_unit', 'Y')] < 5000)]

plt.bar(x=comparison_2.index, height=comparison_2[('price_per_unit', 'Y')], color='#FF9700')
plt.xlabel('Product')
plt.ylabel('Price Difference')
plt.title('Price Difference by Product and Prime')
plt.xticks([])
plt.axhline(0, linestyle='--', linewidth=1, color='gray')
plt.show()

"""### 6. age & gender"""

df['gender'].value_counts()

df['age_grp'].value_counts()

"""#### prime 기준 gender 시각화"""

plt.figure(figsize = (10,10))
plt.subplots_adjust(wspace=0.5)

# employee
plt.subplot(1,2,1)
plt.title("Employee", size=15)
ax = sns.countplot(x = 'gender', hue = 'prime_yn', data = employee)
ax.patches[0].set_facecolor('#0062CD')
ax.patches[1].set_facecolor('#0062CD')
ax.patches[2].set_facecolor('#FF9700')
ax.patches[3].set_facecolor('#FF9700')
legend_labels = ['N', 'Y']
legend_colors = ['#0062CD', '#FF9700']
patches = [mpatches.Patch(facecolor=color) for color in legend_colors]
legend = plt.legend(patches, legend_labels, title='prime_yn', title_fontsize=12)
plt.setp(legend.get_texts(), fontsize='large')

# customer
plt.subplot(1,2,2)
plt.title("Customer", size=15)
ax = sns.countplot(x = 'gender', hue = 'prime_yn', data = customer)
ax.patches[0].set_facecolor('#0062CD')
ax.patches[1].set_facecolor('#0062CD')
ax.patches[2].set_facecolor('#FF9700')
ax.patches[3].set_facecolor('#FF9700')
legend_labels = ['N', 'Y']
legend_colors = ['#0062CD', '#FF9700']
patches = [mpatches.Patch(facecolor=color) for color in legend_colors]
legend = plt.legend(patches, legend_labels, title='prime_yn', title_fontsize=12)
plt.setp(legend.get_texts(), fontsize='large')

plt.show()

"""- employee에서는 남성의 경우가 여성의 경우보다 프라임 회원 비율이 눈에 띄게 큼
- customer에서 여성의 경우는 오히려 비프라임 회원 비율이 높게 나옴
- 두 데이터에서 성별에 따른 프라임 회원 비율에 차이가 있으므로 분석에 추가

#### prime 기준 age_grp에 따른 구매 특성
"""

# prime 여부, 연령대, 카테고리에 따른 net_order_qty 합에 대한 데이터 프레임 qty_ratio 생성
emp_qty_ratio=employee.groupby(['prime_yn', 'age_grp', 'category'], as_index=False).agg({"net_order_qty": sum})

# 직원데이터에서 프라임, 비프라임 데이터 개수 각각 확인
print(employee[employee['prime_yn']=="Y"].shape[0])
print(employee[employee['prime_yn']=="N"].shape[0])

# 비율 컬럼 생성
emp_qty_ratio['ratio']=0

# 직원 데이터에서 prime 여부에 따라 각각 net_order_qty 합의 비율 채워넣기
for i in range(len(emp_qty_ratio)):
  if emp_qty_ratio['prime_yn'][i]=="Y":
    emp_qty_ratio['ratio'][i]=emp_qty_ratio["net_order_qty"][i]/12421  # 프라임 고객인 경우
  else:
    emp_qty_ratio['ratio'][i]=emp_qty_ratio["net_order_qty"][i]/7384   # 비프라임 고객인 경우

emp_qty_ratio_g=emp_qty_ratio.groupby(['prime_yn', 'age_grp', 'category']).agg({"net_order_qty": sum, "ratio":sum})   # prime 여부, 연령대, 카테고리에 따라 net_order_qty, ratio 컬럼 각각 합하여 비율 확인
emp_qty_ratio_g=emp_qty_ratio_g.sort_values(by=['prime_yn', 'age_grp', 'ratio'], ascending=[False, True, False])      # 비율 오름차순으로 정렬
emp_qty_ratio_g.head(60)

# customer데이터에서 prime 여부, 연령대, 카테고리에 따른 net_order_qty 합에 대한 데이터 프레임 qty_ratio 생성
cus_qty_ratio=customer.groupby(['prime_yn', 'age_grp', 'category'], as_index=False).agg({"net_order_qty": sum})

# 일반 고객 데이터에서 프라임, 비프라임 데이터 개수 각각 확인
print(customer[customer['prime_yn']=="Y"].shape[0])  # 12233
print(customer[customer['prime_yn']=="N"].shape[0])  # 13837

# 비율 컬럼 생성
cus_qty_ratio['ratio']=0

# 일반 고객 데이터에서 prime 여부에 따라 각각 net_order_qty 합의 비율 채워넣기
for i in range(len(cus_qty_ratio)):
  if cus_qty_ratio['prime_yn'][i]=="Y":
    cus_qty_ratio['ratio'][i]=cus_qty_ratio["net_order_qty"][i]/12233  # 프라임 고객인 경우
  else:
    cus_qty_ratio['ratio'][i]=cus_qty_ratio["net_order_qty"][i]/13837  # 비프라임 고객인 경우

cus_qty_ratio_g=cus_qty_ratio.groupby(['prime_yn', 'age_grp', 'category']).agg({"net_order_qty": sum, "ratio":sum}) # prime 여부, 연령대, 카테고리에 따라 net_order_qty, ratio 컬럼 각각 합하여 비율 확인
cus_qty_ratio_g=cus_qty_ratio_g.sort_values(by=['prime_yn', 'age_grp', 'ratio'], ascending=[False, True, False])    # 비율 오름차순으로 정렬
cus_qty_ratio_g.head(60)

"""##4. 모델링

###0. 머신에 들어갈 데이터 전처리
"""

df = df.drop(columns = ['scd', 'product_name', 'order_date', 'holiday', 'set_yn'])
df['prime_yn'] = df['prime_yn'].map({'N': 0, 'Y': 1})

employee = df[df['employee_yn'] == 'Y']
customer = df[df['employee_yn'] == 'N']

"""### 1. 모델선택

####01. employee
"""

train_input = employee.drop(columns = ['prime_yn', 'employee_yn'])
train_input['category'] = train_input['category'].astype('str')
train_input['age_grp'] = train_input['age_grp'].astype('str')
train_input = pd.get_dummies(train_input)
train_target = employee['prime_yn']

models = {'DT' : DecisionTreeClassifier(),
          'RF' : RandomForestClassifier(),
          'SVM' : svm.SVC(),
          'SGD' : SGDClassifier(),
          'LR' : LogisticRegression(),
          #'CAT' : CatBoostClassifier(),
          'NB' : GaussianNB(),
          'XGB' : XGBClassifier()}

# employee
model = models.values()
name = models.keys()

for model, name in zip(model, name):
    print('---------- 사용하는 알고리즘',name,'----------')
    for score in ['accuracy','precision','recall','f1']:
        print(score)
        print('--')
        scored = cross_val_score(model, train_input, train_target, scoring = score, cv=5)
        print(scored)
    print('f1_score mean : ', np.mean(scored))

train_input = employee.drop(columns = ['prime_yn', 'employee_yn']).reset_index(drop = True)
train_target = employee['prime_yn'].reset_index(drop = True)

is_holdout = False
n_splits = 5
iterations = 3000
patience = 50

cv = KFold(n_splits = n_splits, shuffle = True)

scores = []
models = []

for train, valid in cv.split(train_input):
    print("="*50)
    preds = []

    model = CatBoostClassifier(iterations = iterations, eval_metric= "F1", cat_features = ['gender', 'age_grp', 'category'])
    model.fit(train_input.iloc[train], train_target[train],
            eval_set=[(train_input.iloc[valid], train_target[valid])],
            early_stopping_rounds = patience ,
            verbose = 100
        )

    models.append(model)
    scores.append(model.get_best_score()["validation"]["F1"])
    if is_holdout:
        break

print(scores)
print(np.mean(scores))

"""####02. customer"""

train_input = customer.drop(columns = ['prime_yn', 'employee_yn'])
train_input['category'] = train_input['category'].astype('str')
train_input['age_grp'] = train_input['age_grp'].astype('str')
train_input = pd.get_dummies(train_input)
train_target = customer['prime_yn']

models = {'DT' : DecisionTreeClassifier(),
          'RF' : RandomForestClassifier(),
          'SVM' : svm.SVC(),
          'SGD' : SGDClassifier(),
          'LR' : LogisticRegression(),
          #'CAT' : CatBoostClassifier(),
          'NB' : GaussianNB(),
          'XGB' : XGBClassifier()}

# customer
model = models.values()
name = models.keys()

for model, name in zip(model, name):
    print('---------- 사용하는 알고리즘',name,'----------')
    for score in ['accuracy','precision','recall','f1']:
        print(score)
        print('--')
        scored = cross_val_score(model, train_input, train_target, scoring = score, cv=5)
        print(scored)
    print('f1_score mean : ', np.mean(scored))

train_input = customer.drop(columns = ['prime_yn', 'employee_yn']).reset_index(drop = True)
train_target = customer['prime_yn'].reset_index(drop = True)

is_holdout = False
n_splits = 5
iterations = 3000
patience = 50

cv = KFold(n_splits = n_splits, shuffle = True)

scores = []
models = []

for train, valid in cv.split(train_input):
    print("="*50)
    preds = []

    model = CatBoostClassifier(iterations = iterations, eval_metric= "F1", cat_features = ['gender', 'age_grp', 'category'])
    model.fit(train_input.iloc[train], train_target[train],
            eval_set=[(train_input.iloc[valid], train_target[valid])],
            early_stopping_rounds = patience ,
            verbose = 100
        )

    models.append(model)
    scores.append(model.get_best_score()["validation"]["F1"])
    if is_holdout:
        break

print(scores)
print(np.mean(scores))

"""###2. XGBOOST Hyperparameter Tuning"""

seed = 2019

"""####01. employee"""

train_emp = employee.drop(columns = ['prime_yn', 'employee_yn'])
train_emp['category'] = train_emp['category'].astype('str')
train_emp['age_grp'] = train_emp['age_grp'].astype('str')
train_emp = pd.get_dummies(train_emp)
predictors = train_emp.columns
target_emp = employee['prime_yn']

def modelfit_e(alg, train, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=100):

    # get new n_estimator
    if useTrainCV:
        xgb_param = alg.get_xgb_params()
        xgtrain = xgb.DMatrix(train[predictors].values, label = target_emp.values)
        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round = alg.get_params()['n_estimators'], nfold=cv_folds,
                          metrics='error', early_stopping_rounds=early_stopping_rounds)
        alg.set_params(n_estimators=cvresult.shape[0])
        print(alg)

    # Fit the algorithm on the data
    alg.fit(train[predictors],target_emp, eval_metric='error')

    #Predict training set:
    train_predictions = alg.predict(train[predictors])
    train_predprob = alg.predict_proba(train[predictors])[:,1]

    #Print model report:
    print("\nModel Report")
    print("f1 :", metrics.f1_score(target_emp.values, train_predictions))

"""#####1) learning_rate와 estimator 수를 고정
- learning_rate = 0.1
- n_estimators = 1000
"""

xgb1 = XGBClassifier(
    learning_rate =0.1,
    n_estimators=1000,
    max_depth=5,
    min_child_weight=1,
    gamma=0,
    subsample=0.8,
    colsample_bytree=0.8,
    objective= 'binary:logistic',
    nthread=-1,
    scale_pos_weight=1,
    seed=seed
)
modelfit_e(xgb1, train_emp, predictors)

"""##### 2) max_depth와 min_child_weight를 튜닝한다."""

param_test1 = {
 'max_depth':range(3,10,3),
 'min_child_weight':range(1,6,2)
}
gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1,
                                                  n_estimators=1000,
                                                  max_depth=5,
                                                  min_child_weight=1,
                                                  gamma=0,
                                                  subsample=0.8,
                                                  colsample_bytree=0.8,
                                                  objective= 'binary:logistic',
                                                  nthread=-1,
                                                  scale_pos_weight=1, seed=seed),
param_grid = param_test1, scoring='f1',n_jobs=-1, cv=5, verbose=10)
gsearch1.fit(train_emp[predictors], target_emp)
gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_

"""##### 3) Gamma를 튜닝한다."""

param_test2 = {
 'gamma':[i/10.0 for i in range(0,5)]
}
gsearch2 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,
                                                  n_estimators=1000,
                                                  max_depth=6,
                                                  min_child_weight=1,
                                                  gamma=0,
                                                  subsample=0.8,
                                                  colsample_bytree=0.8,
                                                  objective= 'binary:logistic',
                                                  thread=-1,
                                                  scale_pos_weight=1,
                                                  seed=seed),
                        param_grid = param_test2, scoring='f1', n_jobs=-1, cv=5)
gsearch2.fit(train_emp[predictors], target_emp)
gsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_

"""##### 4) subsample and colsample_bytree를 튜닝한다."""

param_test3 = {
 'subsample':[i/10.0 for i in range(6,10)],
 'colsample_bytree':[i/10.0 for i in range(6,10)]
}
gsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,
                                                  n_estimators=1000,
                                                  max_depth=6,
                                                  min_child_weight=1,
                                                  gamma=0.4,
                                                  subsample=0.8,
                                                  colsample_bytree=0.8,
                                                  objective= 'binary:logistic',
                                                  thread=-1,
                                                  scale_pos_weight=1,
                                                  seed=seed),
                        param_grid = param_test3, scoring='f1', n_jobs=-1, cv=5, verbose=10)
gsearch3.fit(train_emp[predictors], target_emp)
gsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_

"""##### 4-2) subsample 추가 튜닝하기"""

param_test4 = {
 'subsample':[i/100.0 for i in range(80,100)],
}
gsearch4 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,
                                                  n_estimators=1000,
                                                  max_depth=6,
                                                  min_child_weight=1,
                                                  gamma=0.4,
                                                  subsample=0.9,
                                                  colsample_bytree=0.9,
                                                  objective= 'binary:logistic',
                                                  thread=-1,
                                                  scale_pos_weight=1,
                                                  seed=seed),
                        param_grid = param_test4, scoring='f1', n_jobs=-1, cv=5, verbose=10)
gsearch4.fit(train_emp[predictors], target_emp)
gsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_

"""##### 5) Regularization Parameter 튜닝"""

param_test5 = {
 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]
}
gsearch5 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,
                                                  n_estimators=1000,
                                                  max_depth=6,
                                                  min_child_weight=1,
                                                  gamma=0.4,
                                                  subsample=0.96,
                                                  colsample_bytree=0.9,
                                                  objective= 'binary:logistic',
                                                  thread=-1,
                                                  scale_pos_weight=1,
                                                  seed=seed),
                        param_grid = param_test5, scoring='f1', n_jobs=-1, cv=5, verbose=10)
gsearch5.fit(train_emp[predictors], target_emp)
gsearch5.cv_results_, gsearch5.best_params_, gsearch5.best_score_

"""##### 6) Learning Rate 감소"""

xgb_e = XGBClassifier(
    learning_rate =0.015,
    n_estimators=1000,
    max_depth=6,
    min_child_weight=1,
    gamma=0.4,
    reg_alpha=1e-05,
    subsample=0.96,
    colsample_bytree=0.9,
    objective= 'binary:logistic',
    nthread=-1,
    scale_pos_weight=1,
    seed=seed
)
modelfit_e(xgb_e, train_emp, predictors)

"""####02. customer"""

train_cus = customer.drop(columns = ['prime_yn', 'employee_yn'])
train_cus['category'] = train_cus['category'].astype('str')
train_cus['age_grp'] = train_cus['age_grp'].astype('str')
train_cus = pd.get_dummies(train_cus)
predictors = train_cus.columns
target_cus = customer['prime_yn']

def modelfit_c(alg, train, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=100):

    # get new n_estimator
    if useTrainCV:
        xgb_param = alg.get_xgb_params()
        xgtrain = xgb.DMatrix(train[predictors].values, label=target_cus.values)
        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,
                          metrics='error', early_stopping_rounds=early_stopping_rounds)
        alg.set_params(n_estimators=cvresult.shape[0])
        print(alg)

    # Fit the algorithm on the data
    alg.fit(train[predictors],target_cus, eval_metric='error')

    #Predict training set:
    train_predictions = alg.predict(train[predictors])
    train_predprob = alg.predict_proba(train[predictors])[:,1]

    #Print model report:
    print("\nModel Report")
    print("f1 :", metrics.f1_score(target_cus.values, train_predictions))

"""##### 1) learning_rate와 estimator 수를 고정
- learning_rate = 0.1
- n_estimators = 1000
"""

xgb1 = XGBClassifier(
    learning_rate =0.1,
    n_estimators=1000,
    max_depth=5,
    min_child_weight=1,
    gamma=0,
    subsample=0.8,
    colsample_bytree=0.8,
    objective= 'binary:logistic',
    nthread=-1,
    scale_pos_weight=1,
    seed=seed
)
modelfit_c(xgb1, train_cus, predictors)

"""#####2) max_depth와 min_child_weight를 튜닝"""

param_test1 = {
 'max_depth':range(3,10,3),
 'min_child_weight':range(1,6,2)
}
gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1,
                                                  n_estimators=1000,
                                                  max_depth=5,
                                                  min_child_weight=1,
                                                  gamma=0,
                                                  subsample=0.8,
                                                  colsample_bytree=0.8,
                                                  objective= 'binary:logistic',
                                                  nthread=-1,
                                                  scale_pos_weight=1, seed=seed),
param_grid = param_test1, scoring='f1',n_jobs=-1, cv=5, verbose=10)
gsearch1.fit(train_cus[predictors],target_cus)
gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_

"""#####3) Gamma를 튜닝"""

param_test2 = {
 'gamma':[i/10.0 for i in range(0,5)]
}
gsearch2 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,
                                                  n_estimators=1000,
                                                  max_depth=9,
                                                  min_child_weight=1,
                                                  gamma=0,
                                                  subsample=0.8,
                                                  colsample_bytree=0.8,
                                                  objective= 'binary:logistic',
                                                  thread=-1,
                                                  scale_pos_weight=1,
                                                  seed=seed),
                        param_grid = param_test2, scoring='f1', n_jobs=-1, cv=5, verbose = 10)
gsearch2.fit(train_cus[predictors],target_cus)
gsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_

"""#####4) subsample과 colsample_bytree를 튜닝"""

param_test3 = {
 'subsample':[i/10.0 for i in range(6,10)],
 'colsample_bytree':[i/10.0 for i in range(6,10)]
}
gsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,
                                                  n_estimators=1000,
                                                  max_depth=9,
                                                  min_child_weight=1,
                                                  gamma=0.2,
                                                  subsample=0.8,
                                                  colsample_bytree=0.8,
                                                  objective= 'binary:logistic',
                                                  thread=-1,
                                                  scale_pos_weight=1,
                                                  seed=seed),
                        param_grid = param_test3, scoring='f1', n_jobs=-1, cv=5, verbose=10)
gsearch3.fit(train_cus[predictors],target_cus)
gsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_

"""#####4-2)subsample 추가 튜닝"""

param_test4 = {
 'subsample':[i/100.0 for i in range(80,100)],
}
gsearch4 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,
                                                  n_estimators=1000,
                                                  max_depth=9,
                                                  min_child_weight=1,
                                                  gamma=0.2,
                                                  subsample=0.9,
                                                  colsample_bytree=0.8,
                                                  objective= 'binary:logistic',
                                                  thread=-1,
                                                  scale_pos_weight=1,
                                                  seed=seed),
                        param_grid = param_test4, scoring='f1', n_jobs=-1, cv=5, verbose=10)
gsearch4.fit(train_cus[predictors],target_cus)
gsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_

"""#####5) Regularization Parameter 튜닝"""

param_test5 = {
 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]
}
gsearch5 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1,
                                                  n_estimators=1000,
                                                  max_depth=9,
                                                  min_child_weight=1,
                                                  gamma=0.2,
                                                  subsample=0.93,
                                                  colsample_bytree=0.8,
                                                  objective= 'binary:logistic',
                                                  thread=-1,
                                                  scale_pos_weight=1,
                                                  seed=seed),
                        param_grid = param_test5, scoring='f1', n_jobs=-1, cv=5, verbose=10)
gsearch5.fit(train_cus[predictors], target_cus)
gsearch5.cv_results_, gsearch5.best_params_, gsearch5.best_score_

"""##### 6) Learning rate 감소
- 0.1 -> 0.01
"""

xgb_c = XGBClassifier(
    learning_rate =0.01,
    n_estimators=1000,
    max_depth=9,
    min_child_weight=1,
    gamma=0.2,
    reg_alpha=0.01,
    subsample=0.93,
    colsample_bytree=0.8,
    objective= 'binary:logistic',
    nthread=-1,
    scale_pos_weight=1,
    seed=seed
)
modelfit_c(xgb_c, train_cus, predictors)

"""# Test_data
- train과 같은 logic으로 진행하였음으로 주석 간소화

##1. 데이터 불러오기
"""

test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bda_modeling/dataset/tmk_bda_test.csv', index_col = 0).reset_index(drop = True)
test

"""##2. 데이터 파악하기
- 결측치
- 중복값
"""

test.isnull().sum()

test.duplicated().sum()

"""##3. 파생변수 추가

###1) 상품 한 단위 당 가격 추가
- net_order_amt에 로그 스케일링만 적용되었다고 가정
- 가정에 대한 근거는 따로 설명
"""

# 로그 스케일링 가정하고 exponential함수로 역변환
exponential = np.exp(test['net_order_amt'])

# 상품 한 단위 당 가격 = 전체 가격 / 상품 수량
# 따라서 net_order_amt / net_order_qty 를 수행하여 대략적인 상품 한 단위 당 가격을 구함
test['price_per_unit'] = exponential / test['net_order_qty']

"""###2) 상품별 카테고리 추가"""

# 상품별 카테고리를 정리한 csv파일 불러오기
category_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bda_modeling/dataset/category_train.csv', encoding = 'utf8')
category = category_df[['product_name', 'category']].drop_duplicates('product_name').reset_index(drop = True)

category_df_1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bda_modeling/dataset/category_test.csv', encoding = 'cp949', index_col = 0).reset_index(drop = True)
category_1 = category_df_1[['product_name', 'category']].drop_duplicates('product_name').reset_index(drop = True)

display(category)
display(category_1)

category_ = pd.concat([category, category_1], axis=0).drop_duplicates(['product_name']).reset_index(drop=True)
category_

product = list(category_['product_name'])
category = list(category_['category'])

unique_product = dict(zip(product, category))

test['category']=0
for i in range(len(test)):
  if test['product_name'][i] in unique_product.keys():
    test['category'][i]=unique_product[test['product_name'][i]]

test[test['category'] == 0] # 빕스와 오리엔탈에 대해서 오류 발생

test['category'][test['category'] == 0] = 12 # 빕스는 카테고리 12
test['category'][test['product_name'] == 'CJ 오리엔탈드레싱 620g'] = 7 # 오리엔탈드레싱은 카테고리 7

test[test['category'] == 0]

"""###3) 한 번에 구매한 품목의 가지 수 추가"""

# 한 사람이 한 번 주문을 할 때 몇 개의 품목을 구매하는 지 확인
scd_count = test.groupby('scd').count().iloc[:,0].to_frame()
scd_count.columns = ['scd_count']
scd_count

# 기존 데이터프레임과 scd를 기준으로 병합
test = pd.merge(test, scd_count, how = 'left', on = 'scd')
test

"""## Data Dividing"""

test = test.drop(columns = ['scd', 'product_name', 'order_date'])
test['age_grp'] = test['age_grp'].astype('str')
test['category'] = test['category'].astype('str')


test_e = pd.get_dummies(test[test['employee_yn'] == 'Y'].drop(columns = ['employee_yn']))
test_c = pd.get_dummies(test[test['employee_yn'] == 'N'].drop(columns = ['employee_yn']))

display(test_e)
display(test_c)

"""## test 예측"""

test_e.drop(columns='prime_yn', inplace=True)
test_c.drop(columns='prime_yn', inplace=True)

# employee 추가 전처리
test_e['age_grp_1'] = 0
test_e['age_grp_1'] = test_e['age_grp_1'].astype('uint8')
test_e = test_e[['net_order_qty', 'net_order_amt', 'scd_count', 'price_per_unit',
       'gender_F', 'gender_M', 'age_grp_1', 'age_grp_2', 'age_grp_3',
       'age_grp_4', 'age_grp_5', 'age_grp_6', 'category_1', 'category_10',
       'category_11', 'category_12', 'category_13', 'category_14',
       'category_2', 'category_3', 'category_4', 'category_5', 'category_6',
       'category_7', 'category_8', 'category_9']] # train과 같은 컬럼순으로 배열

# customer 추가 전처리
test_c = test_c[['net_order_qty', 'net_order_amt', 'scd_count', 'price_per_unit',
       'gender_F', 'gender_M', 'age_grp_1', 'age_grp_2', 'age_grp_3',
       'age_grp_4', 'age_grp_5', 'age_grp_6', 'category_1', 'category_10',
       'category_11', 'category_12', 'category_13', 'category_14',
       'category_2', 'category_3', 'category_4', 'category_5', 'category_6',
       'category_7', 'category_8', 'category_9']] # train과 같은 컬럼순으로 배열

print('------ employee ------')
display(test_e)
print('------ customer ------')
display(test_c)

# employee
test_e['prime_yn'] = xgb_e.predict(test_e)
test_e['prime_yn'] = test_e['prime_yn'].apply(lambda x: 'Y' if x == 1 else 'N')
test_e.head(10)

# customer
test_c['prime_yn'] = xgb_c.predict(test_c)
test_c['prime_yn'] = test_c['prime_yn'].apply(lambda x: 'Y' if x == 1 else 'N')
test_c.head(10)

"""## test 예측 결과 내보내기"""

final_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bda_modeling/dataset/tmk_bda_test.csv', index_col = 0).reset_index(drop = True)
final_test

final_test['prime_yn'] = pd.concat([test_e['prime_yn'], test_c['prime_yn']]).sort_index()

final_test['prime_yn']

# final_test.to_csv('test_MOMA.csv')